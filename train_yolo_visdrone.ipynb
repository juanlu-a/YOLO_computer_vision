{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Tiny YOLO on VisDrone Dataset\n",
    "\n",
    "This notebook walks through the complete process of training a YOLOv8-nano (tiny) model on the VisDrone dataset for drone-based object detection.\n",
    "\n",
    "## About VisDrone Dataset\n",
    "VisDrone is a large-scale benchmark dataset for drone-based computer vision tasks, containing:\n",
    "- 10 object categories (pedestrian, person, car, van, bus, truck, motor, bicycle, awning-tricycle, tricycle)\n",
    "- Images captured from various drones at different heights and angles\n",
    "- Challenging scenarios with small objects, occlusion, and crowded scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "First, we'll install the Ultralytics library which provides the YOLOv8 implementation. We'll also import necessary libraries for data handling and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install ultralytics\n",
    "!pip install roboflow  # Optional: if downloading from Roboflow\n",
    "\n",
    "# Import libraries\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download and Prepare VisDrone Dataset\n",
    "\n",
    "The VisDrone dataset needs to be in YOLO format:\n",
    "- Images in one folder\n",
    "- Labels in another folder (one .txt file per image)\n",
    "- Each label line: `class_id center_x center_y width height` (normalized 0-1)\n",
    "\n",
    "You can download VisDrone from:\n",
    "- Official site: http://aiskyeye.com/\n",
    "- Roboflow Universe (already in YOLO format)\n",
    "- Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.6' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Option 1: Download from Roboflow (pre-formatted for YOLO)\n",
    "# You'll need to create a free account at roboflow.com and get your API key\n",
    "\n",
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"YOUR_API_KEY\")\n",
    "# project = rf.workspace(\"visdrone\").project(\"visdrone-2019\")\n",
    "# dataset = project.version(1).download(\"yolov8\")\n",
    "\n",
    "# Option 2: Manual download\n",
    "# Download from http://aiskyeye.com/ and convert to YOLO format\n",
    "# Place in the following structure:\n",
    "# VisDrone/\n",
    "#   ├── train/\n",
    "#   │   ├── images/\n",
    "#   │   └── labels/\n",
    "#   ├── val/\n",
    "#   │   ├── images/\n",
    "#   │   └── labels/\n",
    "#   └── data.yaml\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = Path('./VisDrone')\n",
    "print(f\"Dataset path: {dataset_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Dataset Configuration File\n",
    "\n",
    "YOLO requires a `data.yaml` file that specifies:\n",
    "- Path to training and validation images\n",
    "- Number of classes\n",
    "- Class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data.yaml for VisDrone dataset\n",
    "data_yaml_content = \"\"\"# VisDrone Dataset Configuration\n",
    "path: ./VisDrone  # dataset root dir\n",
    "train: train/images  # train images (relative to 'path')\n",
    "val: val/images  # val images (relative to 'path')\n",
    "\n",
    "# Classes\n",
    "nc: 10  # number of classes\n",
    "names:\n",
    "  0: pedestrian\n",
    "  1: people\n",
    "  2: bicycle\n",
    "  3: car\n",
    "  4: van\n",
    "  5: truck\n",
    "  6: tricycle\n",
    "  7: awning-tricycle\n",
    "  8: bus\n",
    "  9: motor\n",
    "\"\"\"\n",
    "\n",
    "# Write to file\n",
    "with open('visdrone.yaml', 'w') as f:\n",
    "    f.write(data_yaml_content)\n",
    "    \n",
    "print(\"Created visdrone.yaml configuration file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Explore the Dataset\n",
    "\n",
    "Before training, let's visualize some sample images with their annotations to verify the dataset is correctly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize images with bounding boxes\n",
    "def visualize_sample(image_path, label_path, class_names):\n",
    "    \"\"\"Visualize an image with its YOLO format annotations\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # Read labels\n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, 'r') as f:\n",
    "            labels = f.readlines()\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        for label in labels:\n",
    "            class_id, x_center, y_center, width, height = map(float, label.strip().split())\n",
    "            \n",
    "            # Convert from YOLO format to pixel coordinates\n",
    "            x1 = int((x_center - width/2) * w)\n",
    "            y1 = int((y_center - height/2) * h)\n",
    "            x2 = int((x_center + width/2) * w)\n",
    "            y2 = int((y_center + height/2) * h)\n",
    "            \n",
    "            # Draw rectangle and label\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(img, class_names[int(class_id)], (x1, y1-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Visualize a few samples\n",
    "class_names = ['pedestrian', 'people', 'bicycle', 'car', 'van', \n",
    "               'truck', 'tricycle', 'awning-tricycle', 'bus', 'motor']\n",
    "\n",
    "# Get sample images\n",
    "train_images = list(dataset_path.glob('train/images/*.jpg'))[:3]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for idx, img_path in enumerate(train_images):\n",
    "    label_path = str(img_path).replace('images', 'labels').replace('.jpg', '.txt')\n",
    "    img = visualize_sample(img_path, label_path, class_names)\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "    axes[idx].set_title(f'Sample {idx+1}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize the YOLO Model\n",
    "\n",
    "We'll use YOLOv8n (nano), which is the smallest and fastest YOLO variant - perfect for learning and quick iterations.\n",
    "\n",
    "Model variants (from smallest to largest):\n",
    "- **YOLOv8n** (nano): ~3M parameters, fastest\n",
    "- YOLOv8s (small): ~11M parameters\n",
    "- YOLOv8m (medium): ~26M parameters\n",
    "- YOLOv8l (large): ~44M parameters\n",
    "- YOLOv8x (xlarge): ~68M parameters, most accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained YOLOv8n model\n",
    "# This will download the pretrained weights from Ultralytics\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Display model information\n",
    "print(model.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure Training Parameters\n",
    "\n",
    "Key training hyperparameters:\n",
    "- **epochs**: Number of complete passes through the dataset (50-100 is common)\n",
    "- **imgsz**: Input image size (640 is standard, smaller = faster but less accurate)\n",
    "- **batch**: Batch size (adjust based on GPU memory; -1 for auto)\n",
    "- **device**: GPU device (0 for first GPU, 'cpu' for CPU)\n",
    "- **workers**: Number of dataloader workers\n",
    "- **optimizer**: SGD or Adam (SGD is default and usually better)\n",
    "- **lr0**: Initial learning rate\n",
    "- **augment**: Enable data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_config = {\n",
    "    'data': 'visdrone.yaml',        # path to data config file\n",
    "    'epochs': 50,                   # number of epochs to train\n",
    "    'imgsz': 640,                   # image size (pixels)\n",
    "    'batch': 16,                    # batch size (adjust based on GPU memory)\n",
    "    'device': 0,                    # GPU device (0) or 'cpu'\n",
    "    'workers': 8,                   # number of dataloader workers\n",
    "    'patience': 10,                 # early stopping patience\n",
    "    'save': True,                   # save train checkpoints\n",
    "    'project': 'runs/visdrone',     # project name\n",
    "    'name': 'yolov8n_exp',          # experiment name\n",
    "    'exist_ok': True,               # overwrite existing experiment\n",
    "    'pretrained': True,             # use pretrained weights\n",
    "    'optimizer': 'SGD',             # optimizer (SGD, Adam, AdamW)\n",
    "    'verbose': True,                # verbose output\n",
    "    'seed': 42,                     # random seed for reproducibility\n",
    "    'lr0': 0.01,                    # initial learning rate\n",
    "    'lrf': 0.01,                    # final learning rate factor\n",
    "    'momentum': 0.937,              # SGD momentum\n",
    "    'weight_decay': 0.0005,         # optimizer weight decay\n",
    "    'warmup_epochs': 3.0,           # warmup epochs\n",
    "    'box': 7.5,                     # box loss gain\n",
    "    'cls': 0.5,                     # class loss gain\n",
    "    'dfl': 1.5,                     # DFL loss gain\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train the Model\n",
    "\n",
    "Now we'll start the training process. This will:\n",
    "1. Load the pretrained YOLOv8n weights\n",
    "2. Replace the detection head with one matching our 10 VisDrone classes\n",
    "3. Train on the VisDrone dataset\n",
    "4. Save checkpoints and best model\n",
    "5. Generate training metrics and plots\n",
    "\n",
    "**Note**: Training can take several hours depending on your hardware and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "results = model.train(**training_config)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Results saved to: {results.save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate the Model\n",
    "\n",
    "After training, we'll evaluate the model on the validation set to get metrics like:\n",
    "- **mAP@0.5**: Mean Average Precision at IoU threshold 0.5\n",
    "- **mAP@0.5:0.95**: Mean Average Precision at IoU thresholds from 0.5 to 0.95\n",
    "- **Precision**: True positives / (True positives + False positives)\n",
    "- **Recall**: True positives / (True positives + False negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "metrics = model.val()\n",
    "\n",
    "# Print key metrics\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(f\"mAP@0.5: {metrics.box.map50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95: {metrics.box.map:.4f}\")\n",
    "print(f\"Precision: {metrics.box.mp:.4f}\")\n",
    "print(f\"Recall: {metrics.box.mr:.4f}\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(\"\\nPer-Class mAP@0.5:\")\n",
    "for idx, class_name in enumerate(class_names):\n",
    "    print(f\"  {class_name}: {metrics.box.maps[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Training Results\n",
    "\n",
    "Ultralytics automatically generates training plots including:\n",
    "- Loss curves (box, class, DFL)\n",
    "- Precision-Recall curves\n",
    "- Confusion matrix\n",
    "- F1-score curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training results\n",
    "from IPython.display import Image, display\n",
    "\n",
    "results_dir = Path('runs/visdrone/yolov8n_exp')\n",
    "\n",
    "# Show training curves\n",
    "print(\"Training Results:\")\n",
    "display(Image(filename=str(results_dir / 'results.png')))\n",
    "\n",
    "# Show confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "display(Image(filename=str(results_dir / 'confusion_matrix.png')))\n",
    "\n",
    "# Show PR curve\n",
    "print(\"\\nPrecision-Recall Curve:\")\n",
    "display(Image(filename=str(results_dir / 'PR_curve.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Run Inference on Test Images\n",
    "\n",
    "Now let's use our trained model to detect objects in new images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best trained model\n",
    "best_model = YOLO('runs/visdrone/yolov8n_exp/weights/best.pt')\n",
    "\n",
    "# Run inference on validation images\n",
    "val_images = list(dataset_path.glob('val/images/*.jpg'))[:6]\n",
    "\n",
    "# Predict\n",
    "results = best_model.predict(\n",
    "    source=val_images,\n",
    "    conf=0.25,        # confidence threshold\n",
    "    iou=0.45,         # NMS IoU threshold\n",
    "    save=True,        # save results\n",
    "    project='runs/visdrone',\n",
    "    name='predictions',\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "# Display predictions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, result in enumerate(results[:6]):\n",
    "    # Get the image with plotted boxes\n",
    "    img = result.plot()  # BGR format\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "    axes[idx].set_title(f'Detection {idx+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detection statistics\n",
    "for idx, result in enumerate(results[:3]):\n",
    "    boxes = result.boxes\n",
    "    print(f\"\\nImage {idx+1}: {len(boxes)} objects detected\")\n",
    "    for box in boxes:\n",
    "        cls = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        print(f\"  - {class_names[cls]}: {conf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Export the Model (Optional)\n",
    "\n",
    "You can export the trained model to different formats for deployment:\n",
    "- ONNX: For general deployment\n",
    "- TorchScript: For production with PyTorch\n",
    "- TensorRT: For NVIDIA GPUs (fastest)\n",
    "- CoreML: For iOS devices\n",
    "- TFLite: For mobile/edge devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX format\n",
    "onnx_path = best_model.export(format='onnx')\n",
    "print(f\"Model exported to: {onnx_path}\")\n",
    "\n",
    "# Other export options:\n",
    "# best_model.export(format='torchscript')\n",
    "# best_model.export(format='tensorrt')  # requires TensorRT\n",
    "# best_model.export(format='coreml')    # for iOS\n",
    "# best_model.export(format='tflite')    # for mobile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. **Environment Setup**: Installed Ultralytics YOLO and dependencies\n",
    "2. **Dataset Preparation**: Downloaded and formatted VisDrone for YOLO\n",
    "3. **Configuration**: Created data.yaml with class definitions\n",
    "4. **Exploration**: Visualized sample images with annotations\n",
    "5. **Model Initialization**: Loaded pretrained YOLOv8n weights\n",
    "6. **Training**: Trained the model with optimized hyperparameters\n",
    "7. **Evaluation**: Measured performance on validation set\n",
    "8. **Visualization**: Reviewed training metrics and plots\n",
    "9. **Inference**: Ran predictions on new images\n",
    "10. **Export**: Converted model for deployment\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try different model sizes (yolov8s, yolov8m) for better accuracy\n",
    "- Experiment with hyperparameters (learning rate, batch size, augmentation)\n",
    "- Train for more epochs if validation loss is still decreasing\n",
    "- Fine-tune on specific drone scenarios in your use case\n",
    "- Deploy the model to a real-time application"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
